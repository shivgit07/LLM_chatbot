# -*- coding: utf-8 -*-
"""WEb_scraping.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SJaAdad2ye02J0sYTu22hv55dzTkvmWn

# Web Scraping the contents of a website

### contains only the text

### 1. scraping from a wiki page
"""

import requests
from bs4 import BeautifulSoup

# Define the URL of the Wikipedia page you want to scrape
url = 'https://en.wikipedia.org/wiki/Cosmos'

# Send an HTTP GET request to the URL
response = requests.get(url)

# Check if the request was successful
if response.status_code == 200:
    # Parse the HTML content of the page
    soup = BeautifulSoup(response.text, 'html.parser')

    # Find the content you want to scrape (e.g., the main article text)
    content = soup.find('div', {'class': 'mw-parser-output'})

    # Initialize an empty string to store the scraped text
    scraped_text = ''

    # Extract the text from the content and store it
    for paragraph in content.find_all(['p', 'dl']):
        scraped_text += paragraph.get_text() + '\n'

    # Define the filename for the text file
    filename = 'wikipedia_scraped_data.txt'

    # Write the scraped text to a text file
    with open(filename, 'w', encoding='utf-8') as file:
        file.write(scraped_text)

    print(f'Data has been scraped and saved to {filename}')
else:
    print(f'Failed to retrieve the Wikipedia page. Status code: {response.status_code}')

"""2. scraping from iitb main website main page (text from a particular section only)"""

import requests
from bs4 import BeautifulSoup

# Define the URL of the Wikipedia page you want to scrape
url = 'https://www.iitb.ac.in/'

# Send an HTTP GET request to the URL
response = requests.get(url)

# Check if the request was successful
if response.status_code == 200:
    # Parse the HTML content of the page
    soup = BeautifulSoup(response.text, 'html.parser')

    # Find the content you want to scrape (e.g., the main article text)
    content = soup.find('div', {'class': 'view-content'})

    # Initialize an empty string to store the scraped text
    scraped_text = ''

    # Extract the text from the content and store it
    for paragraph in content.find_all('p'):
        scraped_text += paragraph.get_text() + '\n'

    # Define the filename for the text file
    filename = 'scraped_data_1.txt'

    # Write the scraped text to a text file
    with open(filename, 'w', encoding='utf-8') as file:
        file.write(scraped_text)

    print(f'Data has been scraped and saved to {filename}')
else:
    print(f'Failed to retrieve the Wikipedia page. Status code: {response.status_code}')

# Define the filename where you saved the scraped data
filename = 'scraped_data_1.txt'

# Read and print the text from the file
with open(filename, 'r', encoding='utf-8') as file:
    scraped_text = file.read()
    print(scraped_text)

"""3. Text that is hyperlinked along with normal text."""

import requests
from bs4 import BeautifulSoup

# Define the URL of the Wikipedia page you want to scrape
url = 'https://www.iitb.ac.in/'

# Send an HTTP GET request to the URL
response = requests.get(url)

# Check if the request was successful
if response.status_code == 200:
    # Parse the HTML content of the page
    soup = BeautifulSoup(response.text, 'html.parser')

    # Find the content you want to scrape (e.g., the main article text)
    content = soup.find('div', {'class': 'view-content'})

    # Initialize an empty string to store the scraped text
    scraped_text = ''

    # Extract the text from the content and store it
    for paragraph in content.find_all(['p','a']):
        scraped_text += paragraph.get_text() + '\n'

    # Define the filename for the text file
    filename = 'scraped_data_hplnk.txt'

    # Write the scraped text to a text file
    with open(filename, 'w', encoding='utf-8') as file:
        file.write(scraped_text)

    print(f'Data has been scraped and saved to {filename}')
else:
    print(f'Failed to retrieve the Wikipedia page. Status code: {response.status_code}')

# Define the filename where you saved the scraped data
filename = 'scraped_data_hplnk.txt'

# Read and print the text from the file
with open(filename, 'r', encoding='utf-8') as file:
    scraped_text = file.read()
    print(scraped_text)

"""# All text with their hyperlinks"""

import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin

# Define the URL of the Wikipedia page you want to scrape
url = 'https://www.iitb.ac.in/'

# Send an HTTP GET request to the URL
response = requests.get(url)

# Check if the request was successful
if response.status_code == 200:
    # Parse the HTML content of the page
    soup = BeautifulSoup(response.text, 'html.parser')

    # Find all <a> and <p> tags in the entire document
    links_and_paragraphs = soup.find_all(['a', 'p'])

    # Initialize an empty list to store the extracted data
    extracted_data = []

    base_url = 'https://www.iitb.ac.in'  # Change this to match the base URL of your page

    # Extract the text and hyperlinks from the <a> tags and text from <p> tags
    for item in links_and_paragraphs:
        if item.name == 'a':  # <a> tag for hyperlinks
            hyperlink = item.get('href')  # Get the hyperlink
            text = item.get_text()  # Get the text associated with the hyperlink
            if hyperlink:
                # Check if the hyperlink is an absolute URL or a relative URL
                complete_link = hyperlink if hyperlink.startswith(('http:', 'https:')) else urljoin(base_url, hyperlink)
                extracted_data.append(f'Text: {text}\nLink: {complete_link}')
        elif item.name == 'p':  # <p> tag for paragraphs
            text = item.get_text()  # Get the text from the paragraph
            extracted_data.append(f'Paragraph: {text}')

    # Define the filename for the text file
    filename = 'scraped_data_hplnk_2.txt'

    # Write the extracted data to a text file
    with open(filename, 'w', encoding='utf-8') as file:
        for item in extracted_data:
            file.write(item + '\n')

    print(f'Hyperlinks and paragraphs from the entire page have been scraped and saved to {filename}')
else:
    print(f'Failed to retrieve the Wikipedia page. Status code: {response.status_code}')

# Define the filename where you saved the scraped data
filename = 'scraped_data_hplnk_2.txt'

# Read and print the text from the file
with open(filename, 'r', encoding='utf-8') as file:
    scraped_text = file.read()
    print(scraped_text)