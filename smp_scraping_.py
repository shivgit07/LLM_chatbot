# -*- coding: utf-8 -*-
"""SMP_scraping .ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zNy-oeMd_8ESUgguRGXntjndNvhkSzcM
"""

import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin

# Define the URL of the Wikipedia page you want to scrape
url = 'https://smp.gymkhana.iitb.ac.in/incoming_introduction.php'

# Send an HTTP GET request to the URL
response = requests.get(url)

# Check if the request was successful
if response.status_code == 200:
    # Parse the HTML content of the page
    soup = BeautifulSoup(response.text, 'html.parser')

    # Find all <a> and <p> tags in the entire document
    links_and_paragraphs = soup.find_all(['a', 'p'])

    # Initialize an empty list to store the extracted data
    extracted_data = []

    base_url = 'https://smp.gymkhana.iitb.ac.in/'  # Change this to match the base URL of your page

    # Extract the text and hyperlinks from the <a> tags and text from <p> tags
    for item in links_and_paragraphs:
        if item.name == 'a':  # <a> tag for hyperlinks
            hyperlink = item.get('href')  # Get the hyperlink
            text = item.get_text()  # Get the text associated with the hyperlink
            if hyperlink:
                # Check if the hyperlink is an absolute URL or a relative URL
                complete_link = hyperlink if hyperlink.startswith(('http:', 'https:')) else urljoin(base_url, hyperlink)
                extracted_data.append(f'Text: {text}\nLink: {complete_link}')
        elif item.name == 'p':  # <p> tag for paragraphs
            text = item.get_text()  # Get the text from the paragraph
            extracted_data.append(f'Paragraph: {text}')
        # Find the <div> element with class="section-content"
#    section_content_div = soup.find('div', class_='section-content')

    # Check if the div is found
#    if section_content_div:
        # Extract the text under the div
#        div_text = section_content_div.get_text()
 #       extracted_data.append(f'Div Text: {div_text}')

    # Define the filename for the text file
    filename = 'scraped_data_hplnk_2.txt'

    # Write the extracted data to a text file
    with open(filename, 'w', encoding='utf-8') as file:
        for item in extracted_data:
            file.write(item + '\n')

    print(f'Hyperlinks and paragraphs from the entire page have been scraped and saved to {filename}')
else:
    print(f'Failed to retrieve the Wikipedia page. Status code: {response.status_code}')

# Define the filename where you saved the scraped data
filename = 'scraped_data_hplnk_2.txt'

# Read and print the text from the file
with open(filename, 'r', encoding='utf-8') as file:
    scraped_text = file.read()
    print(scraped_text)

# Define the filename of the text file with links
filename = 'scraped_data_hplnk_2.txt'

# Initialize an empty set to store the unique extracted links
unique_links = set()

# Read the file line by line
with open(filename, 'r', encoding='utf-8') as file:
    lines = file.readlines()
    for line in lines:
        if line.startswith('Link: '):
            link = line[len('Link: '):].strip()
            unique_links.add(link)

# Print the unique extracted links
for link in unique_links:
    print(f'Link: {link}')
    print()  # Separate each link for better readability

"""# Text: Institute Bodies
Link: https://smp.gymkhana.iitb.ac.in/extra_ibs.php
"""

print(unique_links)

## The unique_links list contains some email address too which are not rquired
## below code cell removes them

import re

filtered_set = {item for item in unique_links if item.startswith("https://")}

# Print the filtered list
print(filtered_set)

import requests
from bs4 import BeautifulSoup

# Define a list of URLs for the websites you want to scrape
urls = filtered_set  # Add your target URLs

# Define the filename for the text file
filename = 'scraped_data.txt'

# Iterate over the list of URLs
for url in urls:
    # Send an HTTP GET request to the URL
    response = requests.get(url)

    # Check if the request was successful
    if response.status_code == 200:
        # Parse the HTML content of the page
        soup = BeautifulSoup(response.text, 'html.parser')

        # Find the <div> element with class "section-content"
        section_content_div = soup.find('div', class_='section-content')

        # Find the <h2> element with class "section-heading"
        section_heading_h2 = soup.find('h2', class_='section-heading')

        # Check if both elements are found
        if section_content_div and section_heading_h2:
            # Extract the text from both elements
            extracted_text_content = section_content_div.get_text()
            extracted_text_heading = section_heading_h2.get_text()

            # Write the extracted text to the text file in append mode
            with open(filename, 'a', encoding='utf-8') as file:
                file.write(f'Title: {extracted_text_heading}\n\nText: {extracted_text_content}\n\n---\n')

            print(f'Extracted text and title from {url} have been appended to {filename}')
        else:
            print(f"The 'section-content' div and/or 'section-heading' h2 were not found on {url}.")
    else:
        print(f'Failed to retrieve {url}. Status code: {response.status_code}')

# Define the filename where you saved the scraped data
filename = 'scraped_data.txt'

# Read and print the text from the file
with open(filename, 'r', encoding='utf-8') as file:
    scraped_text = file.read()
    print(scraped_text)

import requests
from bs4 import BeautifulSoup

# Define a list of URLs for the websites you want to scrape
urls = ['https://smp.gymkhana.iitb.ac.in/incoming_scholarships.php']  # Add your target URLs

# Define the filename for the text file
filename = 'scraped_tabular_data.txt'

# Iterate over the list of URLs
for url in urls:
    # Send an HTTP GET request to the URL
    response = requests.get(url)

    # Check if the request was successful
    if response.status_code == 200:
        # Parse the HTML content of the page
        soup = BeautifulSoup(response.text, 'html.parser')

        # Find tables in the HTML content
        tables = soup.find_all('table')

        # Extract and process each table
        for table in tables:
            # Find all rows in the table
            rows = table.find_all('tr')

            # Initialize a list to store table data
            table_data = []

            for row in rows:
                # Find all cells in the row
                cells = row.find_all(['th', 'td'])

                # Extract text from each cell and append to table_data
                row_data = [cell.get_text(strip=True) for cell in cells]
                table_data.append(row_data)

            # Write the table data to the text file in a structured format
            with open(filename, 'a', encoding='utf-8') as file:
                file.write(f'URL: {url}\n')
                for row_data in table_data:
                    file.write('\t'.join(row_data) + '\n')
                file.write('---\n')

            print(f'Table from {url} has been appended to {filename}')
    else:
        print(f'Failed to retrieve {url}. Status code: {response.status_code}')